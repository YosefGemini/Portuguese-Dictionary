{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Extraccion de palabras de Pagina Web para creacion de diccionario"
      ],
      "metadata": {
        "id": "BhIuKXDvE4sa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mW-YAY6CV7m",
        "outputId": "361664fc-ac61-42d2-ec86-d9e4a7f99123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Collecting pyphen\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xlsxwriter\n",
            "  Downloading XlsxWriter-3.1.9-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.11.17)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.14)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.4)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=1f17a9c3ce871b5bd398375f250e17ce185a52b0380f0184c15d9627f174fb2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: xlsxwriter, pyphen, langid, bs4\n",
            "Successfully installed bs4-0.0.2 langid-1.1.6 pyphen-0.14.0 xlsxwriter-3.1.9\n"
          ]
        }
      ],
      "source": [
        "\n",
        "pip install requests bs4 nltk pyphen xlsxwriter langid spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importacion de librerias\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "# import csv\n",
        "import xlsxwriter\n",
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"pt_core_news_sm\")\n",
        "# from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet\n",
        "# from nltk.tokenize import RegexpTokenizer\n",
        "# nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "import pyphen\n",
        "# import langid\n",
        "import string\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2i6bVwsFQCh",
        "outputId": "96e9ff0a-c6a6-4457-8586-f91f7098ab1c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_sm')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vwAR4eCKZVtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separacion en silabas"
      ],
      "metadata": {
        "id": "YsBKRiG-RGcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def portuguese_words_in_silabs(word):\n",
        "\n",
        "  dict_hifenacion = pyphen.Pyphen(lang='pt')\n",
        "\n",
        "  silab = dict_hifenacion.inserted(word)\n",
        "  # print(silab)\n",
        "  return silab\n",
        "  # si deseo retornar en un nuevo arreglo\n",
        "  # return sliab.split('-')"
      ],
      "metadata": {
        "id": "dpusMnVaOXqm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test de Silabilizado"
      ],
      "metadata": {
        "id": "cJ97Y3IpS5NX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# listado de lenguajes disponibles\n",
        "print(\"lenguajes disponibles: \\n\", pyphen.LANGUAGES)\n",
        "# palabra selparada en silabas\n",
        "silab_word = portuguese_words_in_silabs('vagalume')\n",
        "print(\"palabra vagalume en silabas: \\n\", silab_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UawadguS9Fa",
        "outputId": "4d6f82c4-e6e2-4118-fc71-dcb9947f17a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lenguajes disponibles: \n",
            " {'af_ZA': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_af_ZA.dic'), 'af': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_af_ZA.dic'), 'be_BY': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_be_BY.dic'), 'be': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_be_BY.dic'), 'bg_BG': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_bg_BG.dic'), 'bg': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_bg_BG.dic'), 'ca': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_ca.dic'), 'cs_CZ': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_cs_CZ.dic'), 'cs': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_cs_CZ.dic'), 'da_DK': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_da_DK.dic'), 'da': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_da_DK.dic'), 'de_AT': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_de_AT.dic'), 'de': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_de_AT.dic'), 'de_CH': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_de_CH.dic'), 'de_DE': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_de_DE.dic'), 'el_GR': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_el_GR.dic'), 'el': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_el_GR.dic'), 'en_GB': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_en_GB.dic'), 'en': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_en_GB.dic'), 'en_US': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_en_US.dic'), 'eo': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_eo.dic'), 'es': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_es.dic'), 'et_EE': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_et_EE.dic'), 'et': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_et_EE.dic'), 'fr': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_fr.dic'), 'gl': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_gl.dic'), 'hr_HR': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_hr_HR.dic'), 'hr': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_hr_HR.dic'), 'hu_HU': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_hu_HU.dic'), 'hu': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_hu_HU.dic'), 'id_ID': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_id_ID.dic'), 'id': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_id_ID.dic'), 'is': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_is.dic'), 'it_IT': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_it_IT.dic'), 'it': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_it_IT.dic'), 'lt': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_lt.dic'), 'lv_LV': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_lv_LV.dic'), 'lv': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_lv_LV.dic'), 'mn_MN': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_mn_MN.dic'), 'mn': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_mn_MN.dic'), 'nb_NO': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_nb_NO.dic'), 'nb': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_nb_NO.dic'), 'nl_NL': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_nl_NL.dic'), 'nl': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_nl_NL.dic'), 'nn_NO': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_nn_NO.dic'), 'nn': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_nn_NO.dic'), 'pl_PL': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_pl_PL.dic'), 'pl': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_pl_PL.dic'), 'pt_BR': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_pt_BR.dic'), 'pt': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_pt_BR.dic'), 'pt_PT': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_pt_PT.dic'), 'ro_RO': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_ro_RO.dic'), 'ro': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_ro_RO.dic'), 'ru_RU': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_ru_RU.dic'), 'ru': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_ru_RU.dic'), 'sk_SK': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sk_SK.dic'), 'sk': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sk_SK.dic'), 'sl_SI': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sl_SI.dic'), 'sl': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sl_SI.dic'), 'sq_AL': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sq_AL.dic'), 'sq': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sq_AL.dic'), 'sr': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sr.dic'), 'sr_Latn': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sr_Latn.dic'), 'sv': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_sv.dic'), 'te_IN': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_te_IN.dic'), 'te': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_te_IN.dic'), 'th_TH': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_th_TH.dic'), 'th': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_th_TH.dic'), 'uk_UA': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_uk_UA.dic'), 'uk': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_uk_UA.dic'), 'zu_ZA': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_zu_ZA.dic'), 'zu': PosixPath('/usr/local/lib/python3.10/dist-packages/pyphen/dictionaries/hyph_zu_ZA.dic')}\n",
            "palabra vagalume en silabas: \n",
            " va-ga-lu-me\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clasificacion morfologica"
      ],
      "metadata": {
        "id": "emqzLCw_WEE7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Y6zZF7oPUl22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def morfologic_label(word):\n",
        "  #print('face 1')\n",
        "  #wordnet.langs\n",
        "  synsets = wordnet.synsets(word, lang='por')\n",
        "\n",
        "  #print('fase 2',synsets)\n",
        "\n",
        "  if synsets:\n",
        "    #print (\"synsets 0.pos: \", synsets[0].pos())\n",
        "    #print (\"synsets 0: \", synsets[0])\n",
        "    morf = synsets[0].pos()\n",
        "\n",
        "    if (morf == 'n'):return 'Sustantivo'\n",
        "    if (morf == 'v'):return 'Verbo'\n",
        "    if (morf == 'a'):return 'Adjetivo'\n",
        "    if (morf == 'r'):return 'Adverbio'\n",
        "\n",
        "  else:\n",
        "    #word_token = nlp.ad\n",
        "    # etiquetas_pos = nltk.pos_tag(word, lang='por')\n",
        "    #print(etiquetas_pos)\n",
        "    return None"
      ],
      "metadata": {
        "id": "Y0u_o4XGUwCO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Clasificacion morfologica"
      ],
      "metadata": {
        "id": "wN2q5nfoZMqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#print(\"idiomas en wordnet \\n\", wordnet.langs)\n",
        "\n",
        "morfolologic_label_word = morfologic_label('vagalume')\n",
        "\n",
        "print(\"etiqueta morfologica\", morfolologic_label_word)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixRqq6-MXORg",
        "outputId": "ed5976b0-dabb-4686-f845-0d98be6b12b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "etiqueta morfologica Sustantivo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verificacion de Palabra\n"
      ],
      "metadata": {
        "id": "QV9lpTtT--WS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def is_portuguese_word(word):\n",
        "#     # Detectar el idioma principal de la palabra\n",
        "#     lang, confidence = langid.classify(word)\n",
        "\n",
        "#     # Verificar si el idioma detectado es portugués y la confianza es suficientemente alta\n",
        "#     return lang == 'pt' and confidence > 0.5"
      ],
      "metadata": {
        "id": "O8aW1kLB_Ci8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extraccion de palabras"
      ],
      "metadata": {
        "id": "VN1RBm9JLDiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_portuguese_words_2(url):\n",
        "  response = requests.get(url)\n",
        "  html = response.text\n",
        "\n",
        "  soup = BeautifulSoup(html, 'html.parser')\n",
        "  #print('fase 4')\n",
        "\n",
        "  # obtengo el texto de la pagina web\n",
        "  textContent = soup.get_text()\n",
        "\n",
        "  #print('fase 5' , textContent[:20])\n",
        "\n",
        "  nlp = spacy.load(\"pt_core_news_sm\")\n",
        "\n",
        "  doc = nlp(textContent)\n",
        "\n",
        "  stop_words = set(stopwords.words('portuguese'))\n",
        "  #portuguese_word = []\n",
        "  portuguese_dict = []\n",
        "  count = 1\n",
        "\n",
        "\n",
        "\n",
        "  for token in doc:\n",
        "    if not(token.pos_ =='PUNCT' or token.pos_=='SPACE' or token.pos_=='NUM'):\n",
        "      silab = portuguese_words_in_silabs(token.text)\n",
        "      #print(f\"Palabra: {token.text}, POS Tag: {token.pos_}\")\n",
        "      portuguese_dict.append([count, token.text, silab, token.pos_])\n",
        "      count = count + 1\n",
        "\n",
        "  #filtered_words = set(portuguese_dict)\n",
        "\n",
        "  #complete_dict = list(filtered_words)\n",
        "  #result = list(filtered_repeated)\n",
        "  return portuguese_dict\n",
        "\n"
      ],
      "metadata": {
        "id": "4E9x-R7HK_AT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urlTest = 'https://www.dicio.com.br/lista-de-palavras/'\n",
        "portuguese_dict = extract_portuguese_words_2(urlTest)\n",
        "\n",
        "\n",
        "print('Primeras 20 palabras: \\n ',portuguese_dict[:20])\n",
        "print('tamaño de diccionario: \\n ', len(portuguese_dict))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "niROp-1JMIuD",
        "outputId": "61855439-a321-49ff-a5f3-4c119be13af3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 20 palabras: \n",
            "  [[1, 'Lista', 'Lis-ta', 'PROPN'], [2, 'de', 'de', 'ADP'], [3, 'Palavras', 'Pa-la-vras', 'PROPN'], [4, 'Dicio', 'Di-cio', 'PROPN'], [5, 'Dicionário', 'Di-ci-o-ná-rio', 'PROPN'], [6, 'Online', 'On-li-ne', 'PROPN'], [7, 'de', 'de', 'ADP'], [8, 'Português', 'Por-tu-guês', 'PROPN'], [9, 'Dicionário', 'Di-ci-o-ná-rio', 'PROPN'], [10, 'Online', 'On-li-ne', 'PROPN'], [11, 'de', 'de', 'ADP'], [12, 'Português', 'Por-tu-guês', 'PROPN'], [13, 'Dicio', 'Di-cio', 'PROPN'], [14, 'Dicionário', 'Di-ci-o-ná-rio', 'PROPN'], [15, 'Online', 'On-li-ne', 'PROPN'], [16, 'de', 'de', 'ADP'], [17, 'Português', 'Por-tu-guês', 'PROPN'], [18, 'Palavras', 'Pa-la-vras', 'PROPN'], [19, 'com', 'com', 'ADP'], [20, 'Lista', 'Lis-ta', 'PROPN']]\n",
            "tamaño de diccionario: \n",
            "  3771\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def extract_portuguese_words(url):\n",
        "#   # Obtengo el contenido de la Pasgina Web\n",
        "#   #print('test Init')\n",
        "#   response = requests.get(url)\n",
        "#   #print('fase 1')\n",
        "#   html = response.text\n",
        "#   #print('fase 3')\n",
        "\n",
        "\n",
        "#   # uso de BeautfulSoup para el analisis HTML\n",
        "#   soup = BeautifulSoup(html, 'html.parser')\n",
        "#   #print('fase 4')\n",
        "\n",
        "#   # obtengo el texto de la pagina web\n",
        "#   textContent = soup.get_text()\n",
        "#   #print('fase 5')\n",
        "\n",
        "#   # tokenizar el texto en palagras\n",
        "\n",
        "#   #tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "#   words = word_tokenize(textContent, language='portuguese')\n",
        "#   #print(words[:20])\n",
        "#   #print('fase 6')\n",
        "#   # filtrado de palabras\n",
        "\n",
        "#   stop_words = set(stopwords.words('portuguese'))\n",
        "\n",
        "\n",
        "#   #print('fase 7')\n",
        "#   filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in stop_words]\n",
        "\n",
        "#   filtered_repeated = set(filtered_words);\n",
        "\n",
        "#   result = list(filtered_repeated)\n",
        "\n",
        "#   return result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xkL8UuaxF61X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test de extraccion"
      ],
      "metadata": {
        "id": "flt-KCN0LCSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# urlTest = 'https://www.dicio.com.br/lista-de-palavras/'\n",
        "# portuguese_words = extract_portuguese_words(urlTest)\n",
        "\n",
        "\n",
        "# print('Primeras 20 palabras: \\n ',portuguese_words[:20])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPtck58NLKME",
        "outputId": "b1f27874-d88c-43e0-c958-cadd20cc8e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Primeras 20 palabras: \n",
            "  ['zeladoria', 'ócio', 'umidade', 'ventura', 'questionamento', 'joia', 'vício', 'incontinência', 'vereda', 'cognitivo', 'zaragata', 'excludente', 'relevante', 'heroico', 'cobiçapalavras', 'keep', 'consciencioso', 'zero', 'unissex', 'mórbido']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportacion a CSV"
      ],
      "metadata": {
        "id": "Q6WITb5ydhY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"portuguese_dict.csv\", \"w\") as csvfile:\n",
        "\n",
        "#     # Crea un escritor CSV\n",
        "#     writer = csv.writer(csvfile, delimiter=\",\")\n",
        "\n",
        "#     # Escribe el arreglo de arreglos en el archivo CSV\n",
        "#     for row in portuguese_dict:\n",
        "#         writer.writerow(row)"
      ],
      "metadata": {
        "id": "jbWxzjt0dmxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportacion a Excel"
      ],
      "metadata": {
        "id": "28JLK5L2e6nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "workbook = xlsxwriter.Workbook(\"portuguese_dict.xlsx\")\n",
        "\n",
        "# Crea una hoja de cálculo en el libro de trabajo\n",
        "worksheet = workbook.add_worksheet()\n",
        "\n",
        "# Escribe los encabezados en la hoja de cálculo\n",
        "\n",
        "worksheet.write(0, 0, \"Numero de palabra\")\n",
        "worksheet.write(0, 0, \"Palabra\")\n",
        "worksheet.write(0, 1, \"Silabas\")\n",
        "worksheet.write(0, 2, \"Etiqueta Morfologica\")\n",
        "\n",
        "# Escribe el arreglo en la hoja de cálculo\n",
        "for row in range(len(portuguese_dict)):\n",
        "    for col in range(len(portuguese_dict[0])):\n",
        "        worksheet.write(row + 1, col, portuguese_dict[row][col])\n",
        "\n",
        "# Guarda el libro de trabajo de Excel\n",
        "workbook.close()"
      ],
      "metadata": {
        "id": "WvWtzq11e5g4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}